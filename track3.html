<!DOCTYPE html>
<meta charset="utf-8">
<title>Redirecting to UG2+ 2024 Track 3 website</title>
<meta http-equiv="refresh" content="0; URL=https://cvpr2024ug2challenge.github.io/track3.html">
<link rel="canonical" href="https://cvpr2024ug2challenge.github.io/track3.html">
<html lang="en-US" dir="ltr">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!--  
    Document Title
    =============================================
  -->
  <title>UG2+ Challenge</title>
    <!--  
    Favicons
    =============================================
  -->
  <link rel="apple-touch-icon" sizes="57x57" href="assets/images/favicons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="assets/images/favicons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="assets/images/favicons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="assets/images/favicons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="assets/images/favicons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="assets/images/favicons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="assets/images/favicons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="assets/images/favicons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="assets/images/favicons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="assets/images/favicons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="assets/images/favicons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="assets/images/favicons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="assets/images/favicons/favicon-16x16.png">
  <link rel="manifest" href="/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="assets/images/favicons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
    <!--  
    Stylesheets
    =============================================
    
  -->
  <!-- Default stylesheets-->
  <link href="assets/lib/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Template specific stylesheets-->
  <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Volkhov:400i" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800" rel="stylesheet">
  <link href="assets/lib/animate.css/animate.css" rel="stylesheet">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
  <link href="assets/lib/et-line-font/et-line-font.css" rel="stylesheet">
  <link href="assets/lib/flexslider/flexslider.css" rel="stylesheet">
  <link href="assets/lib/owl.carousel/dist/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/lib/owl.carousel/dist/assets/owl.theme.default.min.css" rel="stylesheet">
  <link href="assets/lib/magnific-popup/dist/magnific-popup.css" rel="stylesheet">
  <link href="assets/lib/simple-text-rotator/simpletextrotator.css" rel="stylesheet">
  <!-- Main stylesheet and color file-->
  <link href="assets/css/style.css" rel="stylesheet">
  <link id="color-scheme" href="assets/css/colors/default.css" rel="stylesheet">
</head>
<style>
* {
  box-sizing: border-box;
}

.column {
  float: left;
  width: 24.0%;
  padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>
<body data-spy="scroll" data-target=".onpage-navigation" data-offset="60">
  <a id="ddmenuLink" href="menu_transparent.html">Menu</a>
  <main>
    <div class="page-loader">
      <div class="loader">Loading...</div>
    </div>


    <div class="main">
      <section class="module bg-dark-30 portfolio-page-header" data-background="assets/images/t2bg.jpg" style="padding: 30px 0;">
        <div class="container" style="width:100%">
          <div class="row" style="padding-top: 40px">
            <div class="col-sm-6 col-sm-offset-3">
              <!-- <p style="float: right;"><img height="300" width="300" src="pics/smoke_generator.png"></p> -->
              <h2 class="module-title font-alt" style="margin: 0 0 0px">Track 3: All Weather Semantic Segmentation</h2>
              <h3 class="module-subtitle font-serif" style="margin: 0 0 20px"><a href="https://docs.google.com/forms/d/e/1FAIpQLSepaa-sOJRAgwCG7V-ubR-lmuJKZx6gCj-YftFLCDAmc3CmEA/viewform?usp=sf_link" target="_blank" class="section-scroll btn btn-border-w btn-round">Register for this track</a></h3>
            </div>
          </div>
        </div>
      </section>


      <section class="module-medium" style="padding-bottom: 0px"></section>

      <!-- GT-RAIN Dataset Figure -->
      <div class="container" style="padding-bottom: 20px">
        <figure>
          <img src="assets/images/all_weather_temp.PNG" style="width:75%;height:75%;margin-left:auto;margin-right:auto;display:block;border: none;">
          <figcaption style="width:80%;margin-left:auto;margin-right:auto;display:block;text-align: center;">
            <em>Sample data from the <a href="http://visual.ee.ucla.edu/wstream.htm/">WeatherStream</a> [1]. </em></figcaption>
        </figure>
    </div>


      
        <div class="container">
          <div class="row" style="text-align: justify">
            <div class="col-sm-12 font-nat" style="font-size: 13pt">
              <!--
              <p>Images captured in adverse weather conditions significantly impact the performance of many vision tasks. Rain is a common weather phenomenon that introduces visual degradations to captured images and videos through partial occlusions of objects â€“ in heavy rain, severe occlusion to the background. As most vision algorithms assume clear weather, with no interference of rain, their performance suffers. Deraining is the task of removing such visual degradations so that the images are better suited to the assumptions of downstream vision algorithms, as well as for aesthetic fruition. </p>

              <p>UG<sup>2</sup>+ Track 3 aims to promote the development of novel single image deraining algorithms for real images. The competition will feature diverse and challenging scenarios that include (i) various types of rain conditions (i.e. long and short streaks, various densities and accumulation, with and without rain fog), (ii) large variety of background scenes from urban locations (i.e. buildings, streets, cityscapes) to natural scenery (i.e. forests, plains, hills), (iii) varying degrees of illumination from different times of day, and all of which are captured by (iv) cameras that cover a wide array of resolutions, noise levels, and intrinsic parameters. In a collaboration with the authors of GT-RAIN, we will introduce an additional 15 extra scenes set aside as a benchmark test set. The challenge will be split into three phases (training, validation, and testing), where data corresponding to each phase will be released to the participants. The validation set will consist of 5400 frames covering 18 scenes and the testing set 4500 frames covering 15 scenes. Unlike previous evaluation protocols that were limited to qualitative evaluation on real images, the submissions will be evaluated quantitatively using standard metrics like PSNR and SSIM. The ranking of algorithms will be determined based on evaluation scores on the testing set. The first place winner will be awarded $1000 USD, second place will be awarded $800 USD, and third place $500 USD. </p>
              -->

              <p>Images captured in adverse weather conditions significantly impact the performance of many vision tasks. Common weather phenomenon, including but not limited to rain, snow, fog, introduce visual degradations to captured images and videos. These degradations may include partial to severe occlusions of objects, illumination changes, noise, etc. As most vision algorithms assume clear weather, with no interference of particles present under adverse weather, their performance suffers due to the domain gap.</p>

              <p>Approaches to deal with the domain gap ranges from domain adaptation to data augmentation. Yet, domain adaptation assumes access to the target (adverse weather) data, which compared to data collected in clear weather conditions are on orders of magnitude less. Data augmentation, on the other hand, relies on generative models, e.g., image-to-image translation, and synthetic particle rendering. However, image-to-image translation may introduce artifacts into the resulting images used for augmentation. Synthetic data augmentation involves adding adverse weather particles, e.g. rain streaks and snowflakes, to the input images, which may be captured during clear or even sunny conditions; aside from the illumination mismatch, the particle models may also fail to capture diverse variations of the weather phenomenon: from size of the particles to their appearance under motion and accumulation, which is exceedingly difficult to simulate. Hence, both augmentation schemes may induce a sim2real domain gap where the input distribution of synthetic or generated images do not align with distribution of real images captured under adverse weather. Amongst the many challenges, there also lacks a large-scale dataset with annotations to train and evaluate vision algorithms under adverse conditions.</p>

              <p>Building on the recent automated data engine, WeatherStream [1], and the all-weather (rain, fog, snow, etc.) imagery across the globe that was collected by it, we seek to address the need within the community for a large-scale adverse weather dataset with semantic annotations. In a collaboration with the authors of WeatherStream, we have annotated images within the dataset with semantic segmentation masks and will propose the WeatherStream semantic segmentation benchmark. The dataset is comprised of approximately 202K images with diverse range of scenes, camera parameters, and unique locations spanning over 94 countries across the world (see Figure 7). This benchmark will be the cornerstone of the UG2+ Challenge Track 3 for all weather semantic segmentation.</p>

              <p>This challenge aims to promote the development of novel semantic segmentation algorithms for images captured under adverse weather conditions. The competition will feature diverse and challenging scenarios that include (i) various types of weather conditions, e.g. light and heavy degrees of rain, snow, and fog, (ii) large variety of background scenes from urban locations (i.e. buildings, streets, cityscapes) to natural scenery (i.e. forests, plains, hills), (iii) varying degrees of illumination from different times of day, and all of which are captured by (iv) cameras that cover a wide array of resolutions, noise levels, and intrinsic parameters. The goal of the challenge is to spark innovative and robust approaches for semantic segmentation under adverse conditions. To this end, the challenge will be split into three phases (training, validation, and testing), where data corresponding to each phase will be released to the participants. The submissions will be evaluated quantitatively using standard metrics like mean intersection-over-union (mIOU) and pixel accuracy (pAcc). Participants will submit their results on the validation and testing sets for evaluation, with a limited number of attempts per phase to ensure fairness amongst teams registering for the competition at different times. This limit is set at 50 total submissions for the validation phase and 25 total submissions for the testing phase at 5 submissions per day. The best results for each phase will be reported on the benchmark. The ranking of algorithms will be determined based on evaluation scores on the testing set. Participants are required to submit a fact sheet including description of their method, datasets used, hyperparameters, memory, runtime, and hardware requirements, etc. The fact sheets will be compiled into a final report post challenge to highlight trends and innovations. Participants are encouraged to submit manuscripts detailing their method to the workshop.</p>

              <!-- <p>The logistics of Track 3 is handled seperately. For details regarding the datasets, rules and prizes, please check the <span class="font-serif"><a href="https://codalab.lisn.upsaclay.fr/competitions/7988" target="_blank">CodaLab Page</a></span> of the challenge. </p> -->

                <!-- <ol>
                  <li>(Semi-)Supervised Object Detection in Haze Conditions</li>
                  <li>(Semi-)Supervised Face Detection in Low Light Conditions</li>
                </ol> -->
              
              <!-- <p style="text-align: justify; ">
                <!-- In all two sub-challenges, the participant teams are allowed to use external training data that are not mentioned above, including self-synthesized or self-collected data; -->
                <!-- <b>but they must state so in their submissions ("Method description" section in Codalab)</b>. -->
                <!-- Each leaderboard will be divided into two ranking lists: with and without external data. -->
                <!-- The ranking criteria will be the Mean average precision (mAP) on each testing set, with Interception-of-Union (IoU) threshold as 0.5. -->
                <!-- If the ratio of the intersection of a detected region with an annotated face region is greater than 0.5, a score of 1 is assigned to the detected region, and 0 otherwise. -->
                <!-- When mAPs with IoU as 0.5 are equal, the mAPs with higher IoUs (0.6, 0.7, 0.8) will be compared sequentially. -->
              <!-- </p> -->
              	<!-- <ul> -->

              <!-- <p style="text-align: justify">If you have any questions about this challenge track please feel free to email <a style="color: #337ab7; text-decoration: underline" href="mailto:GTRainChallenge@gmail.com">GTRainChallenge@gmail.com</a></p> -->

              <p style="padding-bottom: 40px"> References:<br>
                [1] Zhang, H., Ba, Y., Yang, E., Mehra, V., Gella, B., Suzuki, A., Pfahnl, A., Chandrappa, C., Wong, A., & Kadambi, A. (2023). WeatherStream: Light Transport Automation of Single Image Deweathering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. <br>  </p>
            </div>
          </div>

        </div>
      </section>
      <hr class="divider-w">

    <a id="ddfooterLink" href="footer.html">Footer</a>
      <div class="scroll-up"><a href="#totop"><i class="fa fa-angle-double-up"></i></a></div>
  </main>
    <!--  
    JavaScripts
    =============================================
    -->
    <script src="assets/lib/jquery/dist/jquery.js"></script>
    <script src="assets/lib/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="assets/lib/wow/dist/wow.js"></script>
    <script src="assets/lib/jquery.mb.ytplayer/dist/jquery.mb.YTPlayer.js"></script>
    <script src="assets/lib/isotope/dist/isotope.pkgd.js"></script>
    <script src="assets/lib/imagesloaded/imagesloaded.pkgd.js"></script>
    <!-- <script src="assets/lib/flexslider/jquery.flexslider.js"></script> -->
    <script src="assets/lib/owl.carousel/dist/owl.carousel.min.js"></script>
    <!-- <script src="assets/lib/smoothscroll.js"></script> -->
    <script src="assets/lib/magnific-popup/dist/jquery.magnific-popup.js"></script>
    <script src="assets/lib/simple-text-rotator/jquery.simple-text-rotator.min.js"></script>
    <script src="assets/js/plugins.js"></script>
    <script src="assets/js/main.js"></script>
    <script src="assets/js/ddmenu.js" type="text/javascript"></script>
    <script src="assets/js/ddfooter.js" type="text/javascript"></script>
</body>
</html>
